{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance testing for OpenAI Models\n",
    "\n",
    "This notebook enables the execution of performance testing of a open ai model. The notebook executes requests to the azure open ai endpoints, gathers metrics and creates a trace log in log analytics for each execution. You can then use kusto queries to analyse the results of the test runs.\n",
    "\n",
    "Each run will have its unique ``runId`` that can be used to retrieve the logs for that specific run.\n",
    "\n",
    "Each run consists on executing all the prompts for a use case, for all the concurrency levels that were configured, for a specific number of runs\n",
    "\n",
    "To execute this notebook you must define the following 2 variables on the notebook file itself:\n",
    " - Use Case -> The use case that you need to test\n",
    " - Deployment names -> the open ai models that will be tested\n",
    "\n",
    "\n",
    "You will also need to fill the .env file that is supplied in the package with the correct values for your environemnt:\n",
    "```\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING=\"<key for your application insights instance>\"\n",
    "AZURE_OPENAI_ENDPOINT=\"<url of the azure open ai instance>\"\n",
    "AZURE_OPENAI_VERSION=\"<open ai version>\"\n",
    "AZURE_OPENAI_KEY=\"<api key for open ai>\"\n",
    "OPENAI_TEMPERATURE=<temperature parameter of the openai call>\n",
    "OPENAI_TOP_P=<top-p parameter of the openai call>\n",
    "OPENAI_FREQUENCY_PENALTY=<frequency parameter of the openai call>\n",
    "OPENAI_PRESENCE_PENALTY=<presence parameter of the openai call>\n",
    "OPENAI_MAX_TOKENS=<max tokens parameter of the openai call>\n",
    "OTEL_SERVICE_NAME=\"<Label to be used on app insights to identify this workload>\"\n",
    "```\n",
    "\n",
    "To setup a use case you need to create a folder for that use case inside the useCases folder.\n",
    "Inside each use case you need to create a .json for each prompt that will be executed.\n",
    "The prompts are json files that already in the structure needed to pass to OpenAI, similar to the this example:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant. You should greet the user in at least 4 languages\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Once everything is configured you can run the notebook.\n",
    "\n",
    "The notebook will execute the following flow:\n",
    "\n",
    "``````\n",
    "for each prompt of the use case:\n",
    "    for each concurrency level:\n",
    "        for each open ai model:\n",
    "            execute concurrently the requests to open ai model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Setup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCase = \"UC1\" # set here the use case you want to test\n",
    "deploymentNames = [\"functionsgpt35\"] # the name of the AI models to test\n",
    "concurrencyLevels = [1, 2, 4, 8, 16, 32, 64] # the concurrency levels to test\n",
    "num_runs = 10 # the number of runs to perform for each prompt and concurrency level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Install dependencies and set up platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==1.2.4\n",
    "%pip install azure-monitor-opentelemetry==1.0.0b16\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "configure_azure_monitor(\n",
    "    connection_string=os.getenv(\"APPLICATION_INSIGHTS_CONNECTION_STRING\")\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Define functions to call open ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runOpenAIChatCompletion(prompt, model, concurrency, runId, useCase, startTime, file):\n",
    "    failed = False\n",
    "    tokens = 0\n",
    "    prompt_tokens = 0\n",
    "    completion_tokens = 0\n",
    "    statusCode = 200\n",
    "    tic = time.perf_counter()\n",
    " \n",
    "    try:\n",
    "        completion  = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            temperature=float(os.getenv(\"OPENAI_TEMPERATURE\")),\n",
    "            max_tokens=int(os.getenv(\"OPENAI_MAX_TOKENS\")),\n",
    "            top_p=float(os.getenv(\"OPENAI_TOP_P\")),\n",
    "            frequency_penalty=float(os.getenv(\"OPENAI_FREQUENCY_PENALTY\")),\n",
    "            presence_penalty=float(os.getenv(\"OPENAI_PRESENCE_PENALTY\")),\n",
    "            stop=None\n",
    "        )\n",
    "        tokens = completion.usage.total_tokens\n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        completion_tokens = completion.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        statusCode = e.status_code\n",
    "        failed = True\n",
    "    toc = time.perf_counter()\n",
    "    logger.info(\"Executed open ai\", extra={\n",
    "        \"model\": model,\n",
    "        \"timeElapsed\": toc-tic,\n",
    "        \"concurrency\": concurrency,\n",
    "        \"failed\": failed,\n",
    "        \"total_tokens\": tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"prompt_tokens\" : prompt_tokens,\n",
    "        \"runId\": str(runId),\n",
    "        \"useCase\": useCase,\n",
    "        \"startTime\": startTime,\n",
    "        \"statusCode\": statusCode,\n",
    "        \"prompt\" : file\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Tests execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for file in os.listdir(f\"useCases/{useCase}\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(f\"useCases/{useCase}/{file}\", \"r\") as f:\n",
    "            content = f.read()\n",
    "            prompts.append({\"file\" : file, \"content\": json.loads(content)})\n",
    " \n",
    "threads = []\n",
    " \n",
    "print(\"Readed prompts:\", len(prompts))\n",
    " \n",
    "runId = uuid.uuid4()\n",
    " \n",
    " \n",
    "startTime = time.time()\n",
    "for concurrency in concurrencyLevels:\n",
    "    for prompt in prompts:\n",
    "        for model in deploymentNames:\n",
    "            for i in range(0,num_runs):\n",
    "                while(len(threads)<concurrency):\n",
    "                    threads.append(threading.Thread(target=runOpenAIChatCompletion,args=[prompt[\"content\"], model, concurrency, runId, useCase, startTime, prompt[\"file\"]]))\n",
    "                    continue\n",
    "                print(f\"RunId: {runId} -- Run {i} -- Model {model} -> Starting {concurrency} threads\")\n",
    "                for thread in threads:\n",
    "                    thread.start()\n",
    "                for thread in threads:\n",
    "                    thread.join()\n",
    "                threads = []\n",
    "                if(concurrency > 32):\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    time.sleep(10)\n",
    " \n",
    "print(\"Done for runid {}\".format(runId))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Get results from log analytics\n",
    "\n",
    "You can retrieve the test results by quering the Log Analytics workspace to where the traces were dumped.\n",
    "Use the runId that was generated earlier to identifiy the test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following queries on the models\n",
    "\n",
    "**Performance monitoring**\n",
    "```sql\n",
    "AppTraces\n",
    "| where Message == \"Executed open ai\"\n",
    "| where tostring(Properties[\"runId\"]) == \"<run id>\"\n",
    "| project model=tostring(Properties[\"model\"]), timeElapsed=todecimal(Properties[\"timeElapsed\"]), concurrency=toint(Properties[\"concurrency\"])\n",
    "| summarize [\"Average time in seconds\"]=avg(timeElapsed), percentile(timeElapsed,25), percentile(timeElapsed,50), percentile(timeElapsed,75), percentile(timeElapsed,90) by model, concurrency\n",
    "| order by concurrency asc, model asc\n",
    "```\n",
    "\n",
    "**Failures count**\n",
    "```sql\n",
    "AppTraces\n",
    "| where Message == \"Executed open ai\"\n",
    "| where tostring(Properties[\"runId\"]) == \"<run id>\"\n",
    "| summarize count() by tostring(Properties[\"runId\"])\n",
    "| project Total=count_, RunId=Properties_runId\n",
    "| join (\n",
    "AppTraces\n",
    "| where Message == \"Executed open ai\"\n",
    "| where tostring(Properties[\"runId\"]) == \"<run id>\"\n",
    "| project model=tostring(Properties[\"model\"]), failed=tobool(Properties[\"failed\"]), concurrency=toint(Properties[\"concurrency\"]), RunId=tostring(Properties[\"runId\"]), errorCode = tostring(Properties[\"statusCode\"])\n",
    "| where failed==true\n",
    "| summarize [\"Number of results\"]=count() by failed, model, concurrency, RunId, errorCode\n",
    "| order by concurrency asc, model asc) on $left.RunId ==  $right.RunId\n",
    "| project model, concurrency,errorCode, percentageFailed = round((['Number of results']/ todecimal(Total))*100,2)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
